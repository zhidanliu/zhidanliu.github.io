<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Song Han</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html" class="current">About&nbsp;me</a></div>
<div class="menu-item"><a href="CV.html">Links</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="talks.html">Talks</a></div>
<div class="menu-category">Coursework</div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="course.html">Courses</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Song Han</h1>
</div>
<table class="imgtable"><tr><td>
<img src="songhan.jpg" alt="songhan" width="224px" height="IMGLINKTARGET" />&nbsp;</td>
<td align="left"><p>Song Han received the Ph.D. degree from Stanford University advised by <a href="http://cva.stanford.edu/billd_webpage_new.html">Prof. Bill Dally</a>. His research focuses on energy-efficient deep learning, at the intersection between machine learning and computer architecture. He proposed <a href="https://arxiv.org/pdf/1510.00149v5.pdf/">Deep Compression</a> that can compress deep neural networks by an order of magnitude without losing the prediction accuracy. He designed <a href="https://arxiv.org/pdf/1602.01528v2.pdf">EIE: Efficient Inference Engine</a>, a hardware architecture that can perform inference directly on the compressed sparse model, which saves memory bandwidth and results in significant speedup and energy saving.  His work has been featured by <a href="http://www.nextplatform.com/2015/12/08/emergent-chip-vastly-accelerates-deep-neural-networks">TheNextPlatform</a>, <a href="http://techemergence.com/a-limitless-pill-for-deep-neural-networks/">TechEmergence</a>, <a href="http://www.embedded-vision.com/industry-analysis/blog/seeing-dark-and-efficiently">Embedded Vision</a> and <a href="https://www.oreilly.com/ideas/compressed-representations-in-the-age-of-big-data">O’Reilly</a>. He led research efforts in model compression and hardware acceleration for deep learning that won the Best Paper Award at ICLR’16 and the Best Paper Award at FPGA’17. Before joining Stanford, Song graduated from Tsinghua University.</p>
</td></tr></table>
<div class="infoblock">
<div class="blockcontent">
<p>I will join MIT EECS as an assistant professor starting summer 2018. I'm looking for PhD students interested in deep learning and computer architecture. Welcome to select me as your portfolio reader. I also have multiple openings for summer interns. If you are interested in working with me during summer 2018, drop me an email at FirstnameLastname [at] mit [dot] edu  with your CV, publication and research proposal; a demo or proof of conept would be a huge plus. </p>
</div></div>
<h2>News</h2>
<ul>
<li><p>Dec 9, 2017: Song to present <a href="https://openreview.net/pdf?id=SkhQHMW0W">Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training</a> at NIPS 2017 Workshop: Deep Learning at Supercomputer Scale, Long Beach. </p>
</li>
</ul>
<ul>
<li><p>Dec 8, 2017: Song to present <a href="https://openreview.net/pdf?id=SkhQHMW0W">Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training</a> at NIPS 2017 Workshop on ML Systems, Long Beach.</p>
</li>
</ul>
<ul>
<li><p>Dec 6, 2017: Yi and Song to present &ldquo;Fast-speed Intelligent Video Analytics&rdquo; at NIPS 2017 demo session, Long Beach.</p>
</li>
</ul>
<ul>
<li><p>Sep 1, 2017: <font color="FireBrick"> Song finished his PhD thesis: </font>  <a href="https://purl.stanford.edu/qf934gh3708">Efficient Methods and Hardware for Deep Learning.</a></p>
</li>
</ul>
<ul>
<li><p>July 26, 2017: Song presented <a href="https://arxiv.org/abs/1705.08922">Exploring the Regularity of Sparse Structure in Convolutional Neural Networks</a> at CVPR&rsquo;17 TMCV workshop, Honolulu.</p>
</li>
</ul>
<ul>
<li><p>June 1, 2017: <font color="FireBrick"> Song passed PhD defense.</font> <a href="https://youtu.be/EKZbdh6xia8">[video].</a> </p>
</li>
</ul>
<ul>
<li><p>April 24, 2017: Song presented <a href="https://arxiv.org/pdf/1607.04381.pdf">Dense-Sparse-Dense training</a>, a regularization technique for deep neural networks, at ICLR&rsquo;17, Toulon, France. <a href="http://songhan.github.io/DSD/">[DSD model zoo]</a> <a href="https://www.dropbox.com/s/sohcocza5eeztwu/DSD%20slides.pdf?dl=0">[slides]</a></p>
</li>
</ul>
<ul>
<li><p>Feb 24 2017: Song received <font color="FireBrick"> Best Paper Award</font> for the paper <a href="https://arxiv.org/pdf/1612.00694.pdf">ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA</a> at International Symposium on Field-Programmable Gate Arrays (FPGA), Monterey, CA.</p>
</li>
</ul>
<ul>
<li><p>Feb 22 2017: Song presents <a href="https://www.dropbox.com/s/p7lvelt0aihrwtl/FPGA%2717%20tutorial%20Song%20Han.pdf?dl=0">Deep Learning &ndash; Tutorial and Recent Trends</a> at FPGA&rsquo;17, Monterey. <a href="https://youtu.be/Q0b-CkejWcc">[video]</a></p>
</li>
</ul>
<ul>
<li><p>Feb 6 2017: <a href="https://openreview.net/pdf?id=HyoST_9xl">DSD: Dense-Sparse-Dense Training for Deep Neural Networks</a> is accepted by International Conference on Learning Representations (ICLR) 2017.</p>
</li>
</ul>
<ul>
<li><p>Feb 6 2017: <a href="https://openreview.net/pdf?id=S1_pAu9xl">Trained Tenary Quantization</a> is accepted by International Conference on Learning Representations (ICLR) 2017.</p>
</li>
</ul>
<ul>
<li><p>Feb 1 2017: Song presented &ldquo;Efficient Methods and Hardware for Deep Learning&rdquo; at Efficient Neural Network Summit, Cadence, San Jose.</p>
</li>
</ul>
<ul>
<li><p>Dec 12 2016: Song presented <a href="https://calendar.csail.mit.edu/events/180585">From Compression to Acceleration: Efficient Methods and Hardware for Deep Learning</a> at MIT, Cambridge.</p>
</li>
</ul>
<ul>
<li><p>Dec 9 2016: Song received <font color="FireBrick"> Best Paper Honorable Mention</font> at <a href="http://allenai.org/plato/emdnn">NIPS&rsquo;16 workshop on Efficient Methods for Deep Neural Networks</a>, Barcelona, Spain.</p>
</li>
</ul>
<ul>
<li><p>Nov 20 2016: &ldquo;ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA&rdquo; has been accepted to appear at <a href="http://www.isfpga.org/index.html">FPGA&rsquo;17</a> as a full paper, it is also selected for oral presentation at <a href="http://allenai.org/plato/emdnn">NIPS&rsquo;16 workshop on Efficient Methods for Deep Neural Networks</a>.</p>
</li>
</ul>
<ul>
<li><p>Oct 28 2016: Song received <a href="http://www.industry-academia.org/Stanford-Workshop-Agenda-2016.html"><font color="FireBrick"> Best Poster Award</font></a> at 2016 Stanford Cloud Workshop for his poster entiled &ldquo;Deep Compression, EIE and DSD:  Deep Learning Model Compression, Acceleration, and Regularization&rdquo;.</p>
</li>
</ul>
<ul>
<li><p>Oct 24 2016: Song presented &ldquo;Deep Compression and EIE: Deep Neural Network Model Compression and Hardware Acceleration&rdquo; at <a href="http://researcher.watson.ibm.com/researcher/view_group.php?id=7424">2016 IBM Research Workshop on Architectures for Cognitive Computing and Datacenters</a>, Yorktown Heights.</p>
</li>
</ul>
<ul>
<li><p>Sep 26 2016: Song presented <a href="https://youtu.be/CrDRr2fxbsg">"Deep Neural Network Model Compression and an Efficient Inference Engine"</a> at <a href="http://conferences.oreilly.com/artificial-intelligence/ai-deep-learning-bots-ny/public/schedule/speakers">O'reilly Artificial Intelligence Conference</a>, New York. </p>
</li>
</ul>
<ul>
<li><p>Sep 26 2016: Welcome Huizi and Chenzhuo joining <a href="http://cva.stanford.edu">CVA lab</a>. </p>
</li>
</ul>
<ul>
<li><p>June 20 2016: Song presented <a href="https://youtu.be/TDoqzeloe-w">"EIE: Efficient Inference Engine on Compressed Deep Neural Network"</a> at <a href="http://isca2016.eecs.umich.edu">International Symposium on Computer Architecture</a>, Seoul, Korea.</p>
</li>
</ul>
<ul>
<li><p>June 10 2016: Song presented <a href="https://www.microsoft.com/en-us/research/video/deep-compression-dsd-training-and-eie-deep-neural-network-model-compression-regularization-and-hardware-acceleration/">"Deep Compression, DSD Training and EIE: deep neural network model compression, regularization and hardware acceleration"</a> at Microsoft Research, Redmond. </p>
</li>
</ul>
<ul>
<li><p>May 4 2016: Song received <a href="http://www.iclr.cc/doku.php?id=iclr2016:main"><font color="FireBrick"> Best Paper Award</font></a> in International Conference on Learning Representations (ICLR), San Juan, Puerto Rico.</p>
</li>
</ul>
<h2>Research Interest</h2>
<p>I'm interested in application-driven,  domain-specific computer architecture research. The end of Dennard scaling makes power become the key constraint. I'm interested in achieving higher efficiency by tailoring the architecture to characteristics of the application domain. My current research center around co-designing efficient algorithms and hardware systems for machine learning, to free AI from the power hungry hardware beasts and democratize AI to cheap mobile devices, and also reduce the cost of running deep learning on data centers. I enjoy the research intersections across machine learning algorithms, computer architecture and VLSI design. </p>
<h2>Research Projects</h2>
<p><br /></p>
<table class="imgtable"><tr><td>
<img src="NIPS.jpg" alt="NIPS" width="250px" height="IMGLINKTARGET" />&nbsp;</td>
<td align="left"><p>Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resource. Conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude by learning only the important connections. This reduced the number of parameters of AlexNet by a factor of 9×, that of VGGNet by 13× without affecting their accuracy. <br /></p>
<p><b>S. Han</b>, J. Pool, J. Tran, W. J. Dally, &ldquo;Learning both Weights and Connections for Efficient Neural Networks&rdquo;, NIPS&rsquo;15. <br />
<a href="https://arxiv.org/pdf/1506.02626v3.pdf">[pdf]</a> <a href="https://www.dropbox.com/s/riu5pp69sz9eufm/NIPS%20poster.pdf?dl=0">[poster]</a> </p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="ICLR.jpg" alt="ICLR" width="250px" height="IMGLINKTARGET" />&nbsp;</td>
<td align="left"><p>Large deep neural network model improves prediction accuracy but results in large demand for memory access, which is 100× more power hungry than ALU operations. &ldquo;Deep Compression&rdquo; introduces a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of deep neural networks. Experimented on Imagenet dataset: AlexNet got compressed by 35×, from 240MB to 6.9MB; VGGNet got compressed by 49×, from 552MB to 11.3MB, without affecting their accuracy. This algorithm helps putting deep learning into mobile App. <br /></p>
<p><b>S. Han</b>, H. Mao, W. J. Dally, &ldquo;Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding&rdquo;, ICLR&rsquo;16. <font color="FireBrick"> Best Paper Award.</font> <br /><a href="https://arxiv.org/pdf/1510.00149v5.pdf">[pdf]</a> <a href="http://songhan.github.io/Deep-Compression-AlexNet/">[model]</a> <a href="https://www.dropbox.com/s/k9to6gorry9yaw9/dc%20poster.pdf?dl=0">[poster]</a> <a href="https://www.dropbox.com/s/jfsq371dkk5tidn/ICLR-Final.pdf?dl=0">[slides]</a> <a href="http://videolectures.net/iclr2016_han_deep_compression/">[video]</a> </p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="ISCA.jpg" alt="ISCA" width="250px" height="IMGLINKTARGET" />&nbsp;</td>
<td align="left"><p>To execute DNNs on inexpensive, low-power embedded platform requires executing compressed, sparse DNNs. EIE is the first hardware accelerator for these highly-efficient networks. EIE exploits weight sparsity, weight sharing, and can skip zero activations from ReLU. Evaluated on nine DNN benchmarks, EIE is 189× and 13× faster, 24,000× and 3,000× more energy efficient than a CPU and GPU respectively. EIE both distributed storage and distributed computation to parallelize a sparsified layer across multiple PEs, which achieves load balance and good scalability. EIE is covered by TheNextPlatform, HackerNews, TechEmergence and Embedded Vision. <br /></p>
<p><b>S. Han</b>, X. Liu, H. Mao, J. Pu, A. Pedram, M. Horowitz, W. J. Dally, &ldquo;EIE: Efficient Inference Engine on Compressed Deep Neural Network&rdquo;, ISCA&rsquo;16. <br />
<a href="https://arxiv.org/pdf/1602.01528.pdf">[pdf]</a> <a href="http://isca2016.eecs.umich.edu/wp-content/uploads/2016/07/4A-1.pdf">[slides]</a> <a href="https://youtu.be/TDoqzeloe-w">[video]</a></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="ESE.jpg" alt="ESE" width="250px" height="IMGLINKTARGET" />&nbsp;</td>
<td align="left"><p>ESE takes the approach of EIE one step further to address not only feedforward neural networks but also recurrent neural networks (RNN and LSTM). The recurrent nature produces complicated data dependency, which is more challenging than feedforward neural nets. To deal with this problem, we designed a data flow that can effectively schedule the complex LSTM operations using multiple EIE cores. ESE also present an effective model compression algorithm for LSTM with hardware efficiency considerations, compressed the LSTM by 20x without hurting accuracy. Implemented on Xilinx XCKU060 FPGA running at 200MHz, ESE has a processing power of 282 GOPS/s working directly on a compressed sparse LSTM network, corresponding to 2.52 TOPS/s on an uncompressed dense network.</p>
<p><b>S. Han</b>, J. Kang, H. Mao, Y. Li, D. Xie, H. Luo, Y. Wang, H. Yang, W. J. Dally &ldquo;ESE: Efficient Speech Recognition Engine for Compressed LSTM&rdquo;, FPGA&rsquo;17. <font color="FireBrick"> Best Paper Award.</font> <br />
<a href="https://arxiv.org/pdf/1612.00694.pdf">[pdf]</a> <a href="https://www.dropbox.com/s/lck3xqvxrqtp81v/ESE.pdf?dl=0">[slides]</a></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="DSD.jpg" alt="DSD" width="250px" height="IMGLINKTARGET" />&nbsp;</td>
<td align="left"><p>A critical issue for training large neural networks is to prevent overfitting while at the same time providing enough model capacity. We propose DSD, a dense-sparse-dense training flow, for regularizing deep neural networks to achieve higher accuracy. DSD training can improve the prediction accuracy of a wide range of neural networks: CNN, RNN and LSTMs on the tasks of image classification, caption generation and speech recognition. DSD training flow produces the same model architecture and doesn't incur any inference time overhead.<br /></p>
<p><b>S. Han</b>, J. Pool, S. Narang, H. Mao, S. Tang, E. Elsen, B. Catanzaro, J. Tran, W. J. Dally, &ldquo;DSD: Regularizing Deep Neural Networks with Dense-Sparse-Dense Training Flow&rdquo;, ICLR&rsquo;17.<br />
<a href="http://songhan.github.io/DSD/">[DSD model zoo]</a> <a href="https://arxiv.org/pdf/1607.04381.pdf">[pdf]</a> <a href="https://www.dropbox.com/s/sohcocza5eeztwu/DSD%20slides.pdf?dl=0">[slides]</a></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="ttq.png" alt="TTQ" width="250px" height="IMGLINKTARGET" />&nbsp;</td>
<td align="left"><p>The deployment of large neural networks models can be difficult for mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, our models are nearly 16× smaller than full-precision models.<br /></p>
<p>C. Zhu, <b>S. Han</b>, H. Mao, W. J. Dally, &ldquo;Trained Ternary Quantization&rdquo;, ICLR&rsquo;17.<br />
<a href="https://github.com/czhu95/ternarynet">[code]</a> <a href="https://arxiv.org/pdf/1612.01064.pdf">[pdf]</a></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="squeezenet.jpg" alt="squeezenet" width="250px" height="IMGLINKTARGET" />&nbsp;</td>
<td align="left"><p>Smaller CNN model is easier to deploy on mobile devices. SqueezeNet is a small CNN architecture that achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Together with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510× smaller than AlexNet), which can fully fit on-chip SRAM, making it easier to deploy on embedded device. <br /></p>
<p>F. Iandola, <b>S. Han</b>, M. Moskewicz, K. Ashraf, W. J. Dally, K. Keutzer, &ldquo;SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and &lt; 0.5MB Model Size&rdquo;, arXiv 16.<br />
<a href="http://songhan.github.io/SqueezeNet-Deep-Compression/">[model1]</a> <a href="http://songhan.github.io/SqueezeNet-Residual/">[model2]</a>  <a href="https://arxiv.org/pdf/1602.07360.pdf">[pdf]</a></p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="winograd.jpg" alt="winograd" width="250px" height="IMGLINKTARGET" />&nbsp;</td>
<td align="left"><p>Winograd’s minimal filtering algorithm and network pruning both reduce the operations in CNNs. Unfortunately, these two methods cannot be combined. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we prune the weights in the ”Winograd domain” to exploit static weight sparsity. Second, we move the ReLU operation into the ”Winograd domain” to improve the sparsity of the transformed activations. On CIFAR-10, our method reduces the number of multiplications in the VGG-nagadomi model by 10.2× with no loss of accuracy.<br /></p>
<p>X. Liu, <b>S. Han</b>, H. Mao, W. J. Dally, &ldquo;Efficient Sparse-Winograd Convolutional Neural Networks&rdquo;, ICLR&rsquo;17 workshop.<br />
<a href="https://openreview.net/pdf?id=r1rqJyHKg">[pdf]</a></p>
</td></tr></table>
<h2>Publications</h2>
<ul>
<li><p><a href="https://openreview.net/pdf?id=HyoST_9xl">DSD: Dense-Sparse-Dense Training for Deep Neural Networks</a> <br />
<b>Song Han</b>, Jeff Pool, Sharan Narang, Huizi Mao, Shijian Tang, Erich Elsen, Bryan Catanzaro, John Tran, William J. Dally<br />
<i>to appear at International Conference on Learning Representations (ICLR), April 2017.</i></p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/pdf?id=S1_pAu9xl">Trained Tenary Quantization</a> <br />
Chenzhuo Zhu, <b>Song Han</b>, Huizi Mao, William J. Dally<br />
<i>to appear at International Conference on Learning Representations (ICLR), April 2017.</i></p>
</li>
</ul>
<ul>
<li><p><a href="">Software-Hardware Co-Design for Efficient Neural Network Acceleration</a><br />
Kaiyuan Guo, <b>Song Han</b>, Song Yao, Yu Wang, Yuan Xie, Huazhong Yang <br />
<i>to appear at Hot Chips special issue of IEEE Micro, March/April 2017</i></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1612.00694.pdf">ESE: Efficient Speech Recognition Engine for Sparse LSTM on FPGA</a><br />
<b>Song Han</b>, Junlong Kang, Huizi Mao, Yubin Li, Dongliang Xie, Hong Luo, Yu Wang, Huazhong Yang, William J. Dally <br />
<i>NIPS workshop on Efficient Methods for Deep Neural Networks (EMDNN), Dec 2016, <font color="FireBrick"> Best Paper Honorable Mention.</font></i> <br />
<i>International Symposium on Field-Programmable Gate Arrays (FPGA), Feb 2017, <font color="FireBrick"> Best Paper Award.</font></i></p>
</li>
</ul>
<ul>
<li><p><a href="https://www.oreilly.com/ideas/compressing-and-regularizing-deep-neural-networks">Compressing and Regularizing Deep Neural Networks, Improving Prediction Accuracy Using Deep Compression and DSD Training</a> <br />
<b>Song Han</b><br />
O’Reilly, Nov 2016.   </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1602.01528v2.pdf">EIE: Efficient Inference Engine on Compressed Deep Neural Network</a> <br />
<b>Song Han</b>, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark Horowitz, William J. Dally <br />
<i>International Symposium on Computer Architecture (ISCA), June 2016</i>; <i>Hotchips, Aug 2016.</i></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1510.00149v5.pdf">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a> <br />
<b>Song Han</b>, Huizi Mao, William J. Dally <br />
<i>NIPS Deep Learning Symposium, December 2015.</i> <br />
<i>International Conference on Learning Representations (ICLR), May 2016, <font color="FireBrick"> Best Paper Award.</font></i></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1506.02626.pdf">Learning both Weights and Connections for Efficient Neural Networks</a><br />
<b>Song Han</b>, Jeff Pool, John Tran, William J. Dally<br />
<i>Advances in Neural Information Processing Systems (NIPS), December 2015.</i></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1602.07360v3.pdf">SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and &lt; 0.5MB Model Size</a><br />
Forrest Iandola, <b>Song Han</b>, Matthew Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer<br />
<i> arXiv 2016.</i></p>
</li>
</ul>
<ul>
<li><p><a href="https://www.computer.org/csdl/proceedings/isvlsi/2016/9039/00/9039a024-abs.html">Angel-Eye: A Complete Design Flow for Mapping CNN onto Customized Hardware</a><br />
Kaiyuan Guo, Lingzhi Sui, Jiantao Qiu, Song Yao, <b>Song Han</b>, Yu Wang, Huazhong Yang<br />
<i>IEEE Computer Society Annual Symposium on VLSI (ISVLSI), July 2016.</i></p>
</li>
</ul>
<ul>
<li><p><a href="">Hardware-friendly Convolutional Neural Network with Even-number Filter Size</a> <br />
Song Yao, <b>Song Han</b>, Kaiyuan Guo, Jianqiao Wangni, Yu Wang, William J. Dally <br />
<i>International Conference on Learning Representations Workshop, May 2016.</i></p>
</li>
</ul>
<h2>Invited Talks</h2>
<ul>
<li><p><a href="https://youtu.be/Q0b-CkejWcc">Deep Learning &ndash; Tutorial and Recent Trends</a> </p>
<ul>
<li><p>Conference tutorial at FPGA&rsquo;17, Monterey. <a href="https://www.dropbox.com/s/p7lvelt0aihrwtl/FPGA%2717%20tutorial%20Song%20Han.pdf?dl=0">[slides]</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://youtu.be/kQAhW9gh6aU">Deep Compression: A Deep Neural Network Compression Pipeline</a></p>
<ul>
<li><p>Conference talk at ICLR, Puerto Rico, May 2016.</p>
</li>
<li><p>GPU Technology Conference (GTC), San Jose, March 2016.</p>
</li>
<li><p>Google, Mountain View, March 2015.</p>
</li>
<li><p>Stanford Computer System Colloquium, January 2016. Video.</p>
</li>
<li><p>Baidu, Beijing, December 2015.</p>
</li>
<li><p>Huawei, Shanghai, December 2015. </p>
</li>
<li><p>Horizon Robotics, Beijing, December 2015.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://youtu.be/TDoqzeloe-w">EIE: Efficient Inference Engine on Compressed Deep Neural Network</a></p>
<ul>
<li><p>Conference talk at ISCA, Korea, June 2016.</p>
</li>
<li><p>Movidius, San Mateo, April 2016.</p>
</li>
<li><p>HP Labs, Palo Alto, February 2016.</p>
</li>
<li><p>Apple, Cupertino, December 2015.</p>
</li>
<li><p>Huawei, Shanghai, December 2015.</p>
</li>
<li><p>HiScene, Shanghai, December 2015.</p>
</li>
<li><p>Stanford SystemX Fall Conference, Stanford, November 2015.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://youtu.be/baZOmGSSUAg">Techniques for Efficient Implementation of Deep Neural Networks</a></p>
<ul>
<li><p>Embedded Vision Alliance Member Meeting, March 2016.</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://youtu.be/CrDRr2fxbsg">Deep Compression, DSD Training and EIE: Deep Neural Network Model Compression, Regularization and Hardware Acceleration</a></p>
<ul>
<li><p>O’Reilly Artificial Intelligence Conference, New York, Sep 2016.</p>
</li>
<li><p>Facebook, Menlo Park, Aug 2016.</p>
</li>
<li><p>Tesla, Palo Alto, Aug 2016.</p>
</li>
<li><p>Xilinx, Santa Clara, Aug 2016.</p>
</li>
<li><p>OpenAI, San Francisco, Aug 2016. Video.</p>
</li>
<li><p>Microsoft Research Asia, Beijing, June 2016.</p>
</li>
<li><p>Microsoft Research, Redmond, June 2016.</p>
</li>
<li><p>Apple, Cupertino, June 2016.</p>
</li>
</ul>

</li>
</ul>
<h2>Education</h2>
<ul>
<li><p>Ph.D. Stanford University, Sep. 2012 to Jun. 2017	</p>
</li>
<li><p>M.S.	Stanford University, Sep. 2012 to Jun. 2014	</p>
</li>
<li><p>B.S.	Tsinghua University, Aug. 2008 to Jul. 2012	</p>
</li>
</ul>
<h2>Services</h2>
<ul>
<li><p>Reviewer for Journal of Machine Learning Research (JMLR)</p>
</li>
<li><p>Reviewer for IEEE Transactions on Neural Networks and Learning Systems (TNNLS) </p>
</li>
<li><p>Reviewer for Computer Vision and Image Understanding (CVIU)</p>
</li>
<li><p>Reviewer for IEEE Journal of Solid State Circuits (JSSCC)</p>
</li>
<li><p>Reviewer for IEEE Micro</p>
</li>
<li><p>Reviewer for IEEE Transactions on Computer-Aided Design of Integrated Circuits &amp; Systems (TCAD)</p>
</li>
<li><p>Reviewer for ACM Journal on Emerging Technologies in Computing Systems (JETC) </p>
</li>
<li><p>Reviewer for IEEE Embedded Systems Letters (ESL)</p>
</li>
<li><p>Reviewer for 30th Annual Conference on Neural Information Processing Systems (NIPS) </p>
</li>
<li><p>Reviewer for 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</p>
</li>
</ul>
<h2>Contact</h2>
<ul>
<li><p>Email: FirstnameLastname [at] cs [dot] stanford [dot] edu</p>
</li>
<li><p>Office: Gate Computer Science Building, Room 216<br /></p>
</li>
<li><p>Address: 353 Serra Mall, Stanford, CA 94305</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2017-12-01 12:13:14 PST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
